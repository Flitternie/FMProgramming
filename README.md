# Resource-Efficient Foundation Model Programming

A modular framework for **cost-efficient inference** in complex multi-modal tasks using _Foundation Model Programs_. This system enables dynamic routing of sub-tasks to the most appropriate foundation model backends, optimizing the trade-off between accuracy and computational cost.

---

## ğŸŒŸ Overview  

This repository implements a neurosymbolic execution system where user queries are compiled into Python-like **Foundation Model Programs**. Each program is composed of generic neural functions (e.g., object detection, VQA) and dynamically chooses among multiple backend models during inference.

Key Features:
- **Streaming VQA support** for binary and open-form tasks  
- **Online policy learning** for dynamic backend selection  
- **Structured REINFORCE + Thompson Sampling** for cost-aware inference  
- **Composable program interface** for multi-modal reasoning

Use Cases:
- Visual Question Answering (VQA)  
- Multi-modal scene understanding  
- Cost-constrained decision-making for FM workflows  

---

## ğŸ—‚ï¸ Project Structure

```
ğŸ“FMP
â”œâ”€â”€ ğŸ“config                  # YAML configuration files for model setup and routing parameters
â”‚   â”œâ”€â”€ binary_vqa.yaml       # FM backend and hyperparameter configurations for Streaming Binary VQA experiments
â”‚   â”œâ”€â”€ binary_vqa_supp.yaml  # FM backend and hyperparameter configurations for Streaming Binary VQA experiments (supplementary)
â”‚   â””â”€â”€ open_form_vqa.yaml    # FM backend and hyperparameter configurations for Streaming Open-Form VQA experiments
â”œâ”€â”€ ğŸ“data                    # Input data and saved retrieval images
â”‚   â”œâ”€â”€ binary_vqa.json       # Dataset annotations and queries for the Streaming Binary VQA task
â”‚   â”œâ”€â”€ ğŸ“open_form_vqa       # Dataset annotations and queries for the Streaming Open-Form VQA task
â”œâ”€â”€ ğŸ“execution               # Core execution modules
â”‚   â”œâ”€â”€ backend.py            # Routing backend & program execution engine
â”‚   â”œâ”€â”€ image_patch.py        # ImagePatch class & spatial reasoning
â”‚   â”œâ”€â”€ models.py             # ObjectDetection, VQA, LLM wrappers
â”‚   â”œâ”€â”€ modules.py            # Initialization & model pooling
â”‚   â””â”€â”€ utils.py              # Shared utilities (e.g., transforms, loaders)
â”œâ”€â”€ ğŸ“routing                 # Routing algorithms & networks
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ algorithms.py         # Reinforcement learning routing logic
â”‚   â””â”€â”€ networks.py           # Neural network models for routing decisions
â”œâ”€â”€ api.key                   # API key for remote models (ignored in .gitignore)
â”œâ”€â”€ environment.yml           # Conda environment specification
â”œâ”€â”€ main_verification.py      # Entry point for running Streaming Binary VQA tasks
â”œâ”€â”€ main_vqa.py               # Entry point for running Streaming Open-Form VQA tasks
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ utils_verification.py     # Task-specific utilities
â”œâ”€â”€ utils_vqa.py              # Task-specific utilities
â””â”€â”€ utils.py                  # Shared utility functions
```


---

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/yourusername/FMP.git
cd FMP
conda env create -f environment.yml
conda activate FMP
```

Download COCO:
```bash
mkdir -p data/coco && cd data/coco
wget http://images.cocodataset.org/zips/train2017.zip
wget http://images.cocodataset.org/zips/val2017.zip
unzip train2017.zip && unzip val2017.zip
```

---

## ğŸ“„ Benchmarks

Download binary VQA and open-form VQA benchmarks:
```bash
cd data
wget https://utexas.box.com/shared/static/rdcykkjg41i2rfo7itna4tvrygbmzkct.json -O binary_vqa.json
wget https://utexas.box.com/shared/static/p62xf6oqrp92zeboj8kyne21mbeej6pv.zip -O open_form_vqa.zip && unzip open_form_vqa.zip
```

---

## ğŸš€ Usage

### Run Streaming Binary VQA
```bash
python main_verification.py \
  --cost_weighting 0.001 0.003 0.005 \
  --config config/binary_vqa.yaml \
  --data data/binary_vqa.json \
  --log logs/
```

| Flag             | Description                                         |
|------------------|-----------------------------------------------------|
| `--cost_weighting` | Trade-off between accuracy and cost.               |
| `--config`         | Program + backend routing YAML.                    |
| `--data`           | JSON file with query/image annotations.            |
| `--log`            | Logging directory.                                 |

### Run Streaming Open-form VQA
```bash
python main_vqa.py \
  --cost_weighting 0.001 0.003 0.005 \
  --config config/open_form_vqa.yaml \
  --data data/open_form_vqa \
  --log logs/
```

| Flag             | Description                                         |
|------------------|-----------------------------------------------------|
| `--cost_weighting` | Trade-off between accuracy and cost.               |
| `--config`         | Program + backend routing YAML.                    |
| `--data`           | Root directory of open-form VQA data               |
| `--log`            | Logging directory.                                 |
| `--type`           | Query categories to evaluate. Default to all.      |


---

### âš™ï¸ Configuration

- Backend model selection, routing policy, and program control flow are defined in `config/`.
- Supports:
  - MMDetection (object detection)
  - HuggingFace / vLLM (LLMs and VLMs)
  - AgentLego (tool-based execution)
  - OpenAI/Remote APIs

---

## ğŸ™ Acknowledgments
- Based on ideas from [ViperGPT](https://github.com/cvlab-columbia/viperGPT), [VisProg](https://github.com/allenai/visprog), and neurosymbolic LLM systems.
- Built using PyTorch, MMDetection, HuggingFace Transformers, and AgentLego.
- Benchmarks adapted from GQA, COCO, and A-OKVQA datasets.
